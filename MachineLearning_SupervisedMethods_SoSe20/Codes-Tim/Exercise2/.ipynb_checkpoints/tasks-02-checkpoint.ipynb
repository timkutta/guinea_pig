{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Supervised Methods\n",
    "# Is Learning Feasible?  +  The Linear Model I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Finite Hypothesis Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a) Load the \"banana.csv\" data set provided in the moodle course. Split it into 50% training and test points. Create $M = 10$ hypotheses $H = \\{h_1, \\dots, h_{10}\\}$ at random by sampling a Gaussian weight vector (with numpy.random.randn): there should be 10 random weight vectors $w_1, w_2, \\dots, w_{10}$, defining the 10 hypotheses $h_i = \\textrm{sign}(w_i^T x)$. Define $g$ as the hypothesis with the lowest in-sample error, i.e., the error on the training set. Output this error.**\n",
    "\n",
    "**This is a simplistic training method based on a finite hypothesis set. Althouth the process is of course a very inefficient learner, note that it is exactly compatible with the learning diagram. Training amounts to solving an optimization problem:**\n",
    "$$ g = \\arg\\min\\big\\{E_{in}(h) \\,\\big|\\, h \\in H\\big\\} $$\n",
    "** Note: The quantity called $N$ in the lecture videos is the size of the training set. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **b) Estimate the out-of-sample error of $g$ by computing the test error. The difference between training and test error is the quantity $\\epsilon$ from the lecture videos. What is the bound on the probability for exceeding this value for $M = 10$ hypotheses? Is the bound meaningful? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **c) Verify empirically that the error on the training set is close to the error on the test set for *all* hypothesis in the set $H$. Output the maximal gap between training and test error. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **d) Increase the number of random hypotheses to M = 1000 and check whether the gap between training and test error increases. Try to interpret the result. In order to come to a conclusion, try replacing the \"banana\" data set with the \"mushroom\" data set, and repeat the experiment multiple times. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a) Apply the *sklearn.linear_model.LinearRegression* method to the \"housing\" data set provided in the moodle course. This is a regression problem with real-valued labels. Split the data set 50/50 into training and test. Train a linear regression model and output the mean squared error on the training set and the test set. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** b) Train a linear regression model on the \"mushroom\" data set, using 50% for training and the other 50% for testing. Note that we are applying a regression technique to a classification problem. Compare training and test error (misclassification rate) to the Perceptron.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** c) Train a linear regression model on the \"banana\" data set. Output training and test error (misclassification rate). **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **d) Add polynomial features of the form $x_1^a x_2^b$ for the banana data set, for degrees $a + b \\leq 7$. Apply the class *sklearn.preprocessing.PolynomialFeatures*. Train the linear regression again and check training and test error. How do the errors change? **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
