{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Supervised Methods\n",
    "# Error and Noise  +  Training Versus Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. k-Nearest-Neighbor Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** a) Load the banana data set, split it 50/50 into training and test set, and apply *k*-nearest-neighbor classification (see http://scikit-learn.org/stable/modules/neighbors.html#classification and http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier), with $k \\in \\{1, 3, 5, 7, 9, 19, 29, 49, 99, 199, 299, 499, 999\\}$. Which value of $k$ works best? Compare to results you obtained previously with other methods on this data set. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **b) Visualize the above *k*-NN classifiers along the lines of this example http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html, but without plotting the data (the 'plt.scatter' command), and with a mesh resolution of 'h = 0.05'. For which values of $k$ do you observe overfitting, underfitting, and reasonable performance? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unbalanced Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A classification problem is called *unbalanced* if the numbers of points differ considerably among the classes. Here we investigate the effect of unbalanced data. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** a) Load the \"breastcancer\" data set from the moodle course. Split it 50/50 into training and test set. Print number of positive and negative training points. Create two further subsets of the training set: remove 90% of the +1 class in the first case, and remove 90% of the -1 class in the second case. In both cases, leave the other class as is.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **b) Train linear regression (misused as a classifier) on both sets. Measure the error rate on the test set, and compare to the error rate when training on the full set. Also check for both cases which *types* of errors the model makes, i.e., which class is wrongly classified how often. Interpret the result. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **c) Re-train the two linear regression models, but this time use the third parameter \"sample_weight\" of the \"fit\" method to give 10 times higher weight to training points of the class that was reduced by factor 10. This means that they count 10 times as much in the in-sample error, which is used for training. Check the error rates as in the previous task and compare the results. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dichotomies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a) Load the \"banana\" data set. Create 100 random linear hypothesis of the form $h(x) = \\text{sign}(w^T x)$ as follows: $w_1 = 10$, $w_2 \\in \\mathcal{N}(0, 1)$ (normal distribution with mean 0 and variance 1). None of these will actually perform very well. Compute the number of different dichotomies realized by the 100 classifiers on the data. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* **b) Repeat the experiment for 1000 and for 10000 hypotheses from the same distribution. Do you observe fast of slow growth of the number of dichotomies? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **c) Stick to 10000 hypotheses. Use subsets of the data of size $\\{2, 4, 8, 16, 32, 64, 128, 256\\}$ to compute the number of dichotomies. Does the number look as if it grows exponentially or rather like a low-order polynomial? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Growth Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a) Consider Perceptrons on $\\mathbb{R}$ and on $\\mathbb{R}^2$. Where is the break point? Construct explicit examples where separation of all dichotomies fails. Guess a general rule for Perceptrons on $\\mathbb{R}^d$. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** b) Consider the input space $\\mathbb{R}^2$ and binary classification with $Y = \\{-1, +1\\}$. On this space consider the hypothesis space $H$ of classifiers of the form\n",
    "$$\n",
    "    h(x) = \\text{sign} \\big( p(x) \\big)\n",
    "$$\n",
    "where $p(x_1, x_2)$ is a polynomial of degree two. The class consists of classifiers with linear, parabolic, ellipsoidal and hyperbolic decision boundaries (solution sets of quadratic equations). Does this class have a break point $k$? If so, can you define or at least bound $k$?**\n",
    "<br/><br/>\n",
    "** Hint: After the feature transformation $(1, x_1, x_2) \\mapsto (1, x_1, x_2, x_1^2, x_1 x_2, x_2^2)$ the classifier is linear. Use the result from part a). **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
