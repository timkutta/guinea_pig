{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Supervised Methods\n",
    "# Is Learning Feasible?  +  The Linear Model I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Finite Hypothesis Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a) Load the \"banana.csv\" data set provided in the moodle course. Split it into 50% training and test points. Create $M = 10$ hypotheses $H = \\{h_1, \\dots, h_{10}\\}$ at random by sampling a Gaussian weight vector (with numpy.random.randn): there should be 10 random weight vectors $w_1, w_2, \\dots, w_{10}$, defining the 10 hypotheses $h_i = \\textrm{sign}(w_i^T x)$. Define $g$ as the hypothesis with the lowest in-sample error, i.e., the error on the training set. Output this error.**\n",
    "\n",
    "**This is a simplistic training method based on a finite hypothesis set. Althouth the process is of course a very inefficient learner, note that it is exactly compatible with the learning diagram. Training amounts to solving an optimization problem:**\n",
    "$$ g = \\arg\\min\\big\\{E_{in}(h) \\,\\big|\\, h \\in H\\big\\} $$\n",
    "** Note: The quantity called $N$ in the lecture videos is the size of the training set. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loadCSV(filename):\n",
    "    f = open(filename)\n",
    "    data = np.loadtxt(f, delimiter=',')\n",
    "    X = data[:, 1:]  #vor dem Komma: Zeilen, nach dem Komma: Spalten\n",
    "    y = data[:, 0]\n",
    "    return X, y\n",
    "\n",
    "X, y = loadCSV(\"banana.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.56169354, -1.1159856 ],\n",
       "       [-0.40297224, -0.48806087],\n",
       "       [ 0.43471191,  1.307098  ],\n",
       "       ...,\n",
       "       [ 1.1302304 ,  1.4797409 ],\n",
       "       [-0.00633513, -1.0014036 ],\n",
       "       [ 0.55423325,  1.1879978 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5300, 2)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X) \n",
    "X.shape  # gibt einem Dimension des Arrays an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.,  1., ..., -1., -1., -1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.56169354, -1.1159856 ],\n",
       "       [-0.40297224, -0.48806087],\n",
       "       [ 0.43471191,  1.307098  ],\n",
       "       ...,\n",
       "       [-0.98758189, -0.19052135],\n",
       "       [ 0.92763843, -0.10172408],\n",
       "       [ 0.81871635,  0.54567215]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_training = X[0:int(len(X)/2),:] #man fängt immer bei 0 an zu zählen; der letzte Eintrag wird nicht mitgenommen\n",
    "X_training  \n",
    "len(X_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2650"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X[int(len(X)/2):,:]\n",
    "X_test\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_training = y[0:int(len(y)/2)]\n",
    "y_test = y[int(len(y)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.76405235,  0.40015721],\n",
       "       [ 0.97873798,  2.2408932 ],\n",
       "       [ 1.86755799, -0.97727788],\n",
       "       [ 0.95008842, -0.15135721],\n",
       "       [-0.10321885,  0.4105985 ],\n",
       "       [ 0.14404357,  1.45427351],\n",
       "       [ 0.76103773,  0.12167502],\n",
       "       [ 0.44386323,  0.33367433],\n",
       "       [ 1.49407907, -0.20515826],\n",
       "       [ 0.3130677 , -0.85409574]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "np.random.seed(0)\n",
    "weights = np.random.randn(10,2)\n",
    "weights\n",
    "#len(weights) #10\n",
    "#len(weights[0,:]) #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12556.886092664019"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(X_training)\n",
    "sum = np.zeros(10)\n",
    "for i in range(0,9):\n",
    "    for n in range(0,N):\n",
    "        sum[i] =+ (np.dot(weights[0,:],X_training[n,:])-y_training[n])**2;  #** bedeutet ^2 \n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.738447582137366"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_h1 = (1/N)*sum\n",
    "E_h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = np.zeros(10) \n",
    "sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **b) Estimate the out-of-sample error of $g$ by computing the test error. The difference between training and test error is the quantity $\\epsilon$ from the lecture videos. What is the bound on the probability for exceeding this value for $M = 10$ hypotheses? Is the bound meaningful? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **c) Verify empirically that the error on the training set is close to the error on the test set for *all* hypothesis in the set $H$. Output the maximal gap between training and test error. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **d) Increase the number of random hypotheses to M = 1000 and check whether the gap between training and test error increases. Try to interpret the result. In order to come to a conclusion, try replacing the \"banana\" data set with the \"mushroom\" data set, and repeat the experiment multiple times. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a) Apply the *sklearn.linear_model.LinearRegression* method to the \"housing\" data set provided in the moodle course. This is a regression problem with real-valued labels. Split the data set 50/50 into training and test. Train a linear regression model and output the mean squared error on the training set and the test set. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** b) Train a linear regression model on the \"mushroom\" data set, using 50% for training and the other 50% for testing. Note that we are applying a regression technique to a classification problem. Compare training and test error (misclassification rate) to the Perceptron.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** c) Train a linear regression model on the \"banana\" data set. Output training and test error (misclassification rate). **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **d) Add polynomial features of the form $x_1^a x_2^b$ for the banana data set, for degrees $a + b \\leq 7$. Apply the class *sklearn.preprocessing.PolynomialFeatures*. Train the linear regression again and check training and test error. How do the errors change? **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
